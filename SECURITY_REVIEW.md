# AGCRN êµí†µ ì˜ˆì¸¡ í”„ë¡œì íŠ¸ ë³´ì•ˆ ì·¨ì•½ì  ë° ê°œì„ ì‚¬í•­ ë¶„ì„

**ë¶„ì„ ë‚ ì§œ**: 2025-11-17
**í”„ë¡œì íŠ¸ ë²„ì „**: 2.0.0
**ë¶„ì„ ë²”ìœ„**: ë³´ì•ˆ ì·¨ì•½ì , ì½”ë“œ í’ˆì§ˆ, ì„¤ì • ê´€ë¦¬

---

## ğŸ“‹ ëª©ì°¨

1. [ìš”ì•½](#ìš”ì•½)
2. [ë³´ì•ˆ ì·¨ì•½ì ](#ë³´ì•ˆ-ì·¨ì•½ì )
3. [ì½”ë“œ í’ˆì§ˆ ë¬¸ì œ](#ì½”ë“œ-í’ˆì§ˆ-ë¬¸ì œ)
4. [ì„¤ì • ë° í™˜ê²½ ë¬¸ì œ](#ì„¤ì •-ë°-í™˜ê²½-ë¬¸ì œ)
5. [ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ê¶Œì¥ì‚¬í•­](#ìš°ì„ ìˆœìœ„ë³„-ê°œì„ -ê¶Œì¥ì‚¬í•­)
6. [ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸](#ë³´ì•ˆ-ì²´í¬ë¦¬ìŠ¤íŠ¸)

---

## ìš”ì•½

### ë°œê²¬ëœ ì£¼ìš” ì´ìŠˆ

| ìœ„í—˜ë„ | ê°œìˆ˜ | ì£¼ìš” í•­ëª© |
|--------|------|-----------|
| **HIGH** | 2 | Pickle Deserialization, Unsafe Model Loading |
| **MEDIUM** | 4 | Path Traversal, ì…ë ¥ ê²€ì¦, ì˜ì¡´ì„± ì·¨ì•½ì , ì—ëŸ¬ í•¸ë“¤ë§ |
| **LOW** | 6 | í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬, ë¡œê¹… ì¼ê´€ì„±, íƒ€ì… ì²´í‚¹ ë“± |

### ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” í•­ëª©

1. **Pickle deserialization ì·¨ì•½ì ** (dataset.py:158)
2. **Unsafe PyTorch model loading** (eval.py:145)
3. **Path traversal ë°©ì§€** (ì—¬ëŸ¬ íŒŒì¼ I/O ë¶€ë¶„)

---

## ë³´ì•ˆ ì·¨ì•½ì 

### ğŸ”´ HIGH ìœ„í—˜ë„

#### 1. Pickle Deserialization ì·¨ì•½ì 

**ìœ„ì¹˜**: `src/dataset.py:158`
**ì·¨ì•½ì  ìœ í˜•**: Arbitrary Code Execution (RCE)
**CVSS Score**: 9.8 (Critical)

**í˜„ì¬ ì½”ë“œ**:
```python
data = np.load(npz_path, allow_pickle=True)
```

**ë¬¸ì œì **:
- `allow_pickle=True`ëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ì†ŒìŠ¤ì˜ íŒŒì¼ì„ ì—­ì§ë ¬í™”í•  ë•Œ ì„ì˜ì˜ ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥
- ê³µê²©ìê°€ ì•…ì˜ì ì¸ npz íŒŒì¼ì„ ì œê³µí•˜ë©´ ì‹œìŠ¤í…œì´ ì†ìƒë  ìˆ˜ ìˆìŒ
- [CWE-502: Deserialization of Untrusted Data](https://cwe.mitre.org/data/definitions/502.html)

**ê°œì„  ë°©ì•ˆ**:
```python
# ë°©ë²• 1: allow_pickle=False ì‚¬ìš© (ê¶Œì¥)
data = np.load(npz_path, allow_pickle=False)

# ë°©ë²• 2: íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
import hashlib

def verify_file_integrity(file_path: Path, expected_hash: str) -> None:
    """íŒŒì¼ í•´ì‹œë¥¼ ê²€ì¦í•˜ì—¬ ë¬´ê²°ì„± í™•ì¸"""
    with open(file_path, 'rb') as f:
        file_hash = hashlib.sha256(f.read()).hexdigest()
    if file_hash != expected_hash:
        raise ValueError(f"File integrity check failed for {file_path}")

# ì‚¬ìš© ì˜ˆì‹œ
verify_file_integrity(npz_path, EXPECTED_HASHES.get(npz_path.name))
data = np.load(npz_path, allow_pickle=False)
```

**ì°¸ê³  ìë£Œ**:
- [NumPy Security Advisory](https://numpy.org/doc/stable/reference/generated/numpy.load.html)
- [Python Pickle Security Issues](https://docs.python.org/3/library/pickle.html#module-pickle)

---

#### 2. Unsafe PyTorch Model Loading

**ìœ„ì¹˜**: `src/eval.py:145`
**ì·¨ì•½ì  ìœ í˜•**: Arbitrary Code Execution (RCE)
**CVSS Score**: 9.8 (Critical)

**í˜„ì¬ ì½”ë“œ**:
```python
checkpoint = torch.load(model_path, map_location='cpu')
```

**ë¬¸ì œì **:
- PyTorch 1.xì˜ `torch.load()`ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pickleì„ ì‚¬ìš©í•˜ì—¬ ì„ì˜ ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥
- ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹œ ìœ„í—˜
- [PyTorch Security Best Practices](https://pytorch.org/docs/stable/notes/serialization.html#security)

**ê°œì„  ë°©ì•ˆ**:
```python
# PyTorch 2.0 ì´ìƒì—ì„œëŠ” weights_only=True ì‚¬ìš© (ê¶Œì¥)
try:
    checkpoint = torch.load(
        model_path,
        map_location='cpu',
        weights_only=True  # PyTorch 2.0+
    )
except TypeError:
    # PyTorch 1.x í˜¸í™˜ì„±
    import warnings
    warnings.warn("PyTorch 1.x detected. Consider upgrading to 2.0+ for better security.")
    checkpoint = torch.load(model_path, map_location='cpu')

# ì¶”ê°€: ëª¨ë¸ íŒŒì¼ ê²€ì¦
def load_safe_checkpoint(model_path: Path, expected_keys: set) -> dict:
    """ì•ˆì „í•˜ê²Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if not model_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {model_path}")

    checkpoint = torch.load(
        model_path,
        map_location='cpu',
        weights_only=True
    )

    # ì˜ˆìƒë˜ëŠ” í‚¤ë§Œ ìˆëŠ”ì§€ ê²€ì¦
    unexpected_keys = set(checkpoint.keys()) - expected_keys
    if unexpected_keys:
        raise ValueError(f"Unexpected keys in checkpoint: {unexpected_keys}")

    return checkpoint

# ì‚¬ìš© ì˜ˆì‹œ
EXPECTED_KEYS = {'model_state_dict', 'optimizer_state_dict', 'epoch', 'loss'}
checkpoint = load_safe_checkpoint(model_path, EXPECTED_KEYS)
```

**ì°¸ê³  ìë£Œ**:
- [PyTorch Save/Load Security](https://pytorch.org/docs/stable/notes/serialization.html#security)

---

### ğŸŸ¡ MEDIUM ìœ„í—˜ë„

#### 3. Path Traversal ì·¨ì•½ì 

**ìœ„ì¹˜**: ì—¬ëŸ¬ íŒŒì¼
**ì·¨ì•½ì  ìœ í˜•**: Path Traversal / Directory Traversal
**CVSS Score**: 7.5 (High)

**ì˜í–¥ë°›ëŠ” ì½”ë“œ**:

1. **src/preprocess.py:111** - CSV íŒŒì¼ ì½ê¸°
```python
df = pd.read_csv(csv_path)  # ê²½ë¡œ ê²€ì¦ ì—†ìŒ
```

2. **src/preprocess.py:456-471** - NPZ íŒŒì¼ ì €ì¥
```python
np.savez(output_path, ...)  # ê²½ë¡œ ê²€ì¦ ì—†ìŒ
sensors_df.to_csv(sensors_path, index=False)
```

3. **src/trainer.py:190-196** - ëª¨ë¸ ì €ì¥
```python
torch.save({...}, save_path)  # ê²½ë¡œ ê²€ì¦ ì—†ìŒ
```

4. **src/trainer.py:216-217** - History JSON ì €ì¥
```python
with open(history_path, 'w') as f:
    json.dump(history, f, indent=2)
```

5. **src/eval.py:125** - ê·¸ë˜í”„ ì €ì¥
```python
plt.savefig(save_path)  # ê²½ë¡œ ê²€ì¦ ì—†ìŒ
```

**ë¬¸ì œì **:
- ì‚¬ìš©ìê°€ `../../etc/passwd` ê°™ì€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ë©´ ì‹œìŠ¤í…œ íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥
- í—ˆê°€ë˜ì§€ ì•Šì€ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì“°ê¸° ê°€ëŠ¥
- [CWE-22: Path Traversal](https://cwe.mitre.org/data/definitions/22.html)

**ê°œì„  ë°©ì•ˆ**:
```python
from pathlib import Path
from typing import List

def validate_file_path(
    file_path: Path,
    allowed_dirs: List[Path],
    must_exist: bool = False
) -> Path:
    """
    íŒŒì¼ ê²½ë¡œê°€ í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ë‚´ì— ìˆëŠ”ì§€ ê²€ì¦

    Args:
        file_path: ê²€ì¦í•  íŒŒì¼ ê²½ë¡œ
        allowed_dirs: í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ëª©ë¡
        must_exist: Trueë©´ íŒŒì¼ì´ ì¡´ì¬í•´ì•¼ í•¨

    Returns:
        ê²€ì¦ëœ ì ˆëŒ€ ê²½ë¡œ

    Raises:
        ValueError: ê²½ë¡œê°€ í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ìš°
        FileNotFoundError: must_exist=Trueì´ê³  íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
    """
    file_path = Path(file_path).resolve()

    # í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ì¤‘ í•˜ë‚˜ì— ì†í•˜ëŠ”ì§€ í™•ì¸
    for allowed_dir in allowed_dirs:
        allowed_dir = Path(allowed_dir).resolve()
        try:
            file_path.relative_to(allowed_dir)
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if must_exist and not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            return file_path
        except ValueError:
            continue

    raise ValueError(
        f"File path '{file_path}' is not in allowed directories: {allowed_dirs}"
    )

# config.pyì— í—ˆìš©ëœ ë””ë ‰í† ë¦¬ ì •ì˜
ALLOWED_DATA_DIRS = [
    PROJECT_ROOT / "data",
    PROJECT_ROOT / "checkpoints",
    PROJECT_ROOT / "results",
    PROJECT_ROOT / "logs"
]

# ì‚¬ìš© ì˜ˆì‹œ
from src.config import ALLOWED_DATA_DIRS

# ì½ê¸°
safe_csv_path = validate_file_path(csv_path, [RAW_DATA_DIR], must_exist=True)
df = pd.read_csv(safe_csv_path)

# ì“°ê¸°
safe_output_path = validate_file_path(output_path, ALLOWED_DATA_DIRS)
safe_output_path.parent.mkdir(parents=True, exist_ok=True)
np.savez(safe_output_path, ...)
```

---

#### 4. ì…ë ¥ ê²€ì¦ ë¶€ì¬

**ìœ„ì¹˜**: ì—¬ëŸ¬ íŒŒì¼
**ì·¨ì•½ì  ìœ í˜•**: Input Validation, DoS
**CVSS Score**: 6.5 (Medium)

##### 4-1. íŒŒì¼ í¬ê¸° ì œí•œ ì—†ìŒ

**ìœ„ì¹˜**: `src/preprocess.py:111`

**í˜„ì¬ ì½”ë“œ**:
```python
df = pd.read_csv(csv_path)
# íŒŒì¼ í¬ê¸° ì œí•œ ì—†ìŒ - ë©”ëª¨ë¦¬ ë¶€ì¡± DoS ê°€ëŠ¥
```

**ê°œì„  ë°©ì•ˆ**:
```python
import os
from pathlib import Path

def load_csv_data(
    csv_path: Path,
    max_size_mb: int = 500,
    chunk_size: int = 10000
) -> pd.DataFrame:
    """
    CSV íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œ (í¬ê¸° ê²€ì¦ í¬í•¨)

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        max_size_mb: ìµœëŒ€ íŒŒì¼ í¬ê¸° (MB)
        chunk_size: ì²­í¬ ë‹¨ìœ„ ì½ê¸° í¬ê¸°

    Returns:
        ë¡œë“œëœ DataFrame

    Raises:
        FileNotFoundError: íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        ValueError: íŒŒì¼ì´ ë„ˆë¬´ í° ê²½ìš°
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"CSV file not found: {csv_path}")

    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = os.path.getsize(csv_path) / (1024 * 1024)
    if file_size_mb > max_size_mb:
        raise ValueError(
            f"File too large: {file_size_mb:.2f}MB (max: {max_size_mb}MB)"
        )

    logger.info(f"Loading CSV file ({file_size_mb:.2f}MB): {csv_path}")

    # ëŒ€ìš©ëŸ‰ íŒŒì¼ì€ ì²­í¬ë¡œ ì½ê¸°
    if file_size_mb > 100:
        logger.info(f"Large file detected, reading in chunks of {chunk_size}")
        chunks = pd.read_csv(csv_path, chunksize=chunk_size)
        df = pd.concat(chunks, ignore_index=True)
    else:
        df = pd.read_csv(csv_path)

    logger.info(f"Loaded {len(df):,} rows")
    return df
```

##### 4-2. ëª…ë ¹í–‰ ì¸ì ê²€ì¦ ì—†ìŒ

**ìœ„ì¹˜**: `train.py:21-33`

**í˜„ì¬ ì½”ë“œ**:
```python
parser.add_argument('--batch_size', type=int, default=BATCH_SIZE)
parser.add_argument('--lr', type=float, default=LEARNING_RATE)
parser.add_argument('--epochs', type=int, default=NUM_EPOCHS)
parser.add_argument('--imputed_weight', type=float, default=0.1)
# ë²”ìœ„ ê²€ì¦ ì—†ìŒ
```

**ê°œì„  ë°©ì•ˆ**:
```python
def validate_args(args) -> argparse.Namespace:
    """
    ëª…ë ¹í–‰ ì¸ì ê²€ì¦

    Args:
        args: argparseë¡œ íŒŒì‹±ëœ ì¸ì

    Returns:
        ê²€ì¦ëœ ì¸ì

    Raises:
        ValueError: ì¸ìê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    errors = []

    # Batch size ê²€ì¦
    if not (1 <= args.batch_size <= 1024):
        errors.append(f"Invalid batch_size: {args.batch_size} (must be 1-1024)")

    # Learning rate ê²€ì¦
    if not (0 < args.lr <= 1.0):
        errors.append(f"Invalid learning rate: {args.lr} (must be 0-1.0)")

    # Epochs ê²€ì¦
    if not (1 <= args.epochs <= 10000):
        errors.append(f"Invalid epochs: {args.epochs} (must be 1-10000)")

    # Imputed weight ê²€ì¦
    if not (0.0 <= args.imputed_weight <= 1.0):
        errors.append(f"Invalid imputed_weight: {args.imputed_weight} (must be 0.0-1.0)")

    # Loss function ê²€ì¦
    valid_losses = ['mse', 'masked_mse', 'masked_mae', 'observed_only']
    if args.loss not in valid_losses:
        errors.append(f"Invalid loss: {args.loss} (must be one of {valid_losses})")

    # Device ê²€ì¦
    if args.device not in ['cuda', 'cpu']:
        errors.append(f"Invalid device: {args.device} (must be 'cuda' or 'cpu')")

    if errors:
        raise ValueError("Argument validation failed:\n" + "\n".join(f"  - {e}" for e in errors))

    return args

# train.pyì—ì„œ ì‚¬ìš©
args = parser.parse_args()
args = validate_args(args)
logger.info(f"Arguments validated: {vars(args)}")
```

---

#### 5. ì˜ì¡´ì„± ë²„ì „ ì·¨ì•½ì 

**ìœ„ì¹˜**: `requirements.txt`
**ì·¨ì•½ì  ìœ í˜•**: Dependency Vulnerabilities
**CVSS Score**: 6.0 (Medium)

**í˜„ì¬ ìƒíƒœ**:
```
torch>=1.9.0,<2.0.0  # ë„ˆë¬´ ë„“ì€ ë²”ìœ„ - ì•Œë ¤ì§„ ì·¨ì•½ì  í¬í•¨ ê°€ëŠ¥
numpy>=1.21.0,<2.0.0
pandas>=1.3.0,<2.0.0
pyyaml>=5.4.0  # ìƒí•œì„  ì—†ìŒ - í˜¸í™˜ì„± ë¬¸ì œ ê°€ëŠ¥
```

**ë¬¸ì œì **:
- ë²„ì „ ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ì–´ ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì ì´ ìˆëŠ” ë²„ì „ ì„¤ì¹˜ ê°€ëŠ¥
- PyYAML 5.4.0ì€ CVE-2020-1747 ë“± ì·¨ì•½ì  ì¡´ì¬
- ìƒí•œì„ ì´ ì—†ìœ¼ë©´ í˜¸í™˜ì„± ë¬¸ì œ ë°œìƒ ê°€ëŠ¥

**ê°œì„  ë°©ì•ˆ**:
```
# requirements.txt ê°œì„  ë²„ì „
# Deep Learning
torch==1.13.1  # ë˜ëŠ” ìµœì‹  ì•ˆì • ë²„ì „ (ë³´ì•ˆ íŒ¨ì¹˜ ì ìš©)
torchvision==0.14.1

# Data Processing
numpy==1.24.3  # CVE í•´ê²°ëœ ë²„ì „
pandas==1.5.3

# Visualization
matplotlib==3.7.1
seaborn==0.12.2

# Progress & Utilities
tqdm==4.65.0
pyyaml>=6.0,<7.0  # CVE-2020-1747, CVE-2020-14343 í•´ê²°

# Testing
pytest==7.3.1
pytest-cov==4.1.0

# Code Quality
black==23.3.0
flake8==6.0.0
isort==5.12.0

# Logging & Monitoring
tensorboard==2.13.0

# Security
safety>=2.3.0  # ì˜ì¡´ì„± ì·¨ì•½ì  ìŠ¤ìº”
bandit>=1.7.0  # ì½”ë“œ ë³´ì•ˆ ìŠ¤ìº”
```

**ë³´ì•ˆ ìŠ¤ìº” ì„¤ì •**:
```bash
# ì˜ì¡´ì„± ì·¨ì•½ì  ìŠ¤ìº”
pip install safety
safety check

# ì½”ë“œ ë³´ì•ˆ ìŠ¤ìº”
pip install bandit
bandit -r src/ -f txt -o bandit_report.txt

# GitHub Actionsì— ë³´ì•ˆ ìŠ¤ìº” ì¶”ê°€
# .github/workflows/security.yml
name: Security Scan
on: [push, pull_request]
jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Safety
        run: |
          pip install safety
          safety check --json
      - name: Run Bandit
        run: |
          pip install bandit
          bandit -r src/
```

**ì°¸ê³  ìë£Œ**:
- [PyYAML CVE-2020-1747](https://nvd.nist.gov/vuln/detail/CVE-2020-1747)
- [Safety - Python Dependency Checker](https://pyup.io/safety/)

---

#### 6. ì—ëŸ¬ í•¸ë“¤ë§ ë¶€ì¬

**ìœ„ì¹˜**: `src/trainer.py`
**ì·¨ì•½ì  ìœ í˜•**: Error Handling
**CVSS Score**: 5.0 (Medium)

**ë¬¸ì œì **:
- ì „ì²´ train() ë©”ì„œë“œì— try-except ì—†ìŒ
- GPU OOM ì—ëŸ¬ ì²˜ë¦¬ ì—†ìŒ
- NaN loss ë°œìƒ ì‹œ ì²˜ë¦¬ ì—†ìŒ
- ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ ì—†ìŒ

**í˜„ì¬ ì½”ë“œ**:
```python
def train_epoch(self) -> float:
    self.model.train()
    # GPU OOM, NaN loss ë“± ì˜ˆì™¸ ì²˜ë¦¬ ì—†ìŒ
    for batch_data in pbar:
        x, y, masks = batch_data
        x = x.to(self.device)  # CUDA OOM ê°€ëŠ¥
        ...
```

**ê°œì„  ë°©ì•ˆ**:
```python
import logging
from typing import Dict, Optional

logger = logging.getLogger(__name__)

class Trainer:
    def train_epoch(self) -> float:
        """í•œ ì—í­ í•™ìŠµ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0

        try:
            pbar = tqdm(self.train_loader, desc="Training")
            for batch_idx, batch_data in enumerate(pbar):
                try:
                    x, y, masks = batch_data
                    x = x.to(self.device)
                    y = y.to(self.device)

                    self.optimizer.zero_grad()
                    output = self.model(x)

                    # Loss ê³„ì‚°
                    loss = self._compute_loss(output, y, masks)

                    # NaN/Inf ì²´í¬
                    if torch.isnan(loss) or torch.isinf(loss):
                        logger.warning(
                            f"Invalid loss at batch {batch_idx}: {loss.item()}, skipping"
                        )
                        continue

                    loss.backward()

                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=5.0
                    )

                    self.optimizer.step()

                    total_loss += loss.item()
                    num_batches += 1
                    pbar.set_postfix({'loss': loss.item()})

                except RuntimeError as e:
                    if "out of memory" in str(e):
                        logger.error(
                            f"GPU OOM at batch {batch_idx}. "
                            "Consider reducing batch size."
                        )
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                        continue
                    else:
                        raise

        except KeyboardInterrupt:
            logger.info("Training interrupted by user")
            raise
        except Exception as e:
            logger.error(f"Training epoch failed: {str(e)}")
            raise

        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss

    def train(
        self,
        num_epochs: int,
        save_path: Optional[Path] = None
    ) -> Dict[str, list]:
        """ë©”ì¸ í•™ìŠµ ë£¨í”„ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)"""
        history = {'train_loss': [], 'val_loss': []}
        self.best_val_loss = float('inf')

        try:
            logger.info(f"Starting training for {num_epochs} epochs")
            logger.info(f"Device: {self.device}")

            for epoch in range(num_epochs):
                try:
                    train_loss = self.train_epoch()
                    val_loss = self.validate()

                    history['train_loss'].append(train_loss)
                    history['val_loss'].append(val_loss)

                    logger.info(
                        f"Epoch {epoch+1}/{num_epochs} - "
                        f"train_loss: {train_loss:.6f}, val_loss: {val_loss:.6f}"
                    )

                    # ëª¨ë¸ ì €ì¥ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
                    if val_loss < self.best_val_loss and save_path:
                        self.best_val_loss = val_loss
                        try:
                            save_path.parent.mkdir(parents=True, exist_ok=True)
                            torch.save({
                                'epoch': epoch,
                                'model_state_dict': self.model.state_dict(),
                                'optimizer_state_dict': self.optimizer.state_dict(),
                                'train_loss': train_loss,
                                'val_loss': val_loss,
                            }, save_path)
                            logger.info(f"âœ“ Saved best model: {save_path}")
                        except IOError as e:
                            logger.error(f"Failed to save model: {e}")
                            # ê³„ì† í•™ìŠµ

                except KeyboardInterrupt:
                    logger.info("Training interrupted by user at epoch {epoch+1}")
                    break

        except Exception as e:
            logger.error(f"Training failed: {str(e)}")
            raise
        finally:
            # History ì €ì¥ ì‹œë„ (ì‹¤íŒ¨í•´ë„ ì—ëŸ¬ ë°œìƒ ì•ˆ í•¨)
            if save_path:
                history_path = save_path.parent / "training_history.json"
                try:
                    with open(history_path, 'w') as f:
                        json.dump(history, f, indent=2)
                    logger.info(f"âœ“ Saved training history: {history_path}")
                except IOError as e:
                    logger.warning(f"Failed to save history: {e}")

        return history
```

---

### ğŸ”µ LOW ìœ„í—˜ë„

#### 7. í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ ë¶€ì¬

**ìœ„ì¹˜**: `src/config.py`
**ì·¨ì•½ì  ìœ í˜•**: Configuration Management
**CVSS Score**: 3.0 (Low)

**ë¬¸ì œì **:
- ëª¨ë“  ì„¤ì •ì´ í•˜ë“œì½”ë”©ë¨ - í™˜ê²½ë³„ ì„¤ì • ë¶ˆê°€
- ë¯¼ê°í•œ ì •ë³´ ê´€ë¦¬ ë°©ë²• ì—†ìŒ
- ê°œë°œ/í”„ë¡œë•ì…˜ í™˜ê²½ êµ¬ë¶„ ì—†ìŒ

**ê°œì„  ë°©ì•ˆ**:

**1. `.env` íŒŒì¼ ì§€ì› ì¶”ê°€**:

```bash
# .env.example (í…œí”Œë¦¿)
# Environment
AGCRN_ENV=development  # development, production, testing

# Device
AGCRN_DEVICE=cuda

# Training
AGCRN_BATCH_SIZE=64
AGCRN_LEARNING_RATE=0.001
AGCRN_NUM_EPOCHS=100

# Data
AGCRN_DATA_DIR=./data
AGCRN_NODE_MODE=raw_id

# Logging
AGCRN_LOG_LEVEL=INFO
AGCRN_LOG_FILE=logs/agcrn.log

# Optional: API Keys (if needed)
# MODEL_API_KEY=your_api_key_here
```

**2. config.py ìˆ˜ì •**:

```python
# src/config.py
from pathlib import Path
from dotenv import load_dotenv
import os
import logging

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# Project structure
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = Path(os.getenv('AGCRN_DATA_DIR', PROJECT_ROOT / "data"))
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
META_DATA_DIR = DATA_DIR / "meta"

# Environment
ENVIRONMENT = os.getenv('AGCRN_ENV', 'development')

# Device
DEVICE = os.getenv('AGCRN_DEVICE',
                   "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu")

# Training
BATCH_SIZE = int(os.getenv('AGCRN_BATCH_SIZE', '64'))
LEARNING_RATE = float(os.getenv('AGCRN_LEARNING_RATE', '0.001'))
NUM_EPOCHS = int(os.getenv('AGCRN_NUM_EPOCHS', '100'))

# Data
NODE_MODE = os.getenv('AGCRN_NODE_MODE', 'raw_id')

# Logging
LOG_LEVEL = os.getenv('AGCRN_LOG_LEVEL', 'INFO')
LOG_FILE = Path(os.getenv('AGCRN_LOG_FILE', PROJECT_ROOT / 'logs' / 'agcrn.log'))

# Validation
if ENVIRONMENT not in ['development', 'production', 'testing']:
    raise ValueError(f"Invalid environment: {ENVIRONMENT}")

if BATCH_SIZE < 1 or BATCH_SIZE > 1024:
    raise ValueError(f"Invalid batch size: {BATCH_SIZE}")

logger.info(f"Environment: {ENVIRONMENT}")
logger.info(f"Device: {DEVICE}")
```

**3. requirements.txtì— ì¶”ê°€**:
```
python-dotenv>=1.0.0
```

---

#### 8. í•˜ë“œì½”ë”©ëœ ì •ë³´

**ìœ„ì¹˜**: `setup.py:28-30`
**ì·¨ì•½ì  ìœ í˜•**: Information Disclosure
**CVSS Score**: 2.0 (Low)

**í˜„ì¬ ì½”ë“œ**:
```python
author="Your Name",
author_email="your.email@example.com",
url="https://github.com/your-username/agcrn-traffic-prediction",
```

**ê°œì„  ë°©ì•ˆ**:
```python
# setup.py
import os

setup(
    name="agcrn-traffic-prediction",
    version="2.0.0",
    author=os.getenv('PACKAGE_AUTHOR', 'AGCRN Team'),
    author_email=os.getenv('PACKAGE_EMAIL', 'contact@example.com'),
    url=os.getenv('PACKAGE_URL', 'https://github.com/junjunjunbong/agcrn-traffic-prediction'),
    ...
)
```

---

#### 9. .gitignore ëˆ„ë½

**ìœ„ì¹˜**: `.gitignore`
**ìœ„í—˜ë„**: LOW

**ë¬¸ì œì **:
- `.env` íŒŒì¼ì´ gitignoreì— ì—†ìŒ
- ë¯¼ê°í•œ ì„¤ì • íŒŒì¼ íŒ¨í„´ ëˆ„ë½
- ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼ íŒŒì¼ ëˆ„ë½

**ê°œì„  ë°©ì•ˆ**:

```gitignore
# .gitignoreì— ì¶”ê°€

# Environment & Secrets
.env
.env.*
!.env.example
*.key
*.pem
*.pfx
credentials.json
secrets.yaml
config.local.yaml

# Security scans
.safety/
bandit_report.txt
bandit_report.json
safety_report.json

# Temporary files
*.tmp
*.temp
.DS_Store

# IDE
.vscode/
.idea/
*.swp
*.swo

# Logs (ë¯¼ê°í•œ ì •ë³´ í¬í•¨ ê°€ëŠ¥)
logs/*.log
!logs/.gitkeep
```

---

## ì½”ë“œ í’ˆì§ˆ ë¬¸ì œ

### 1. ì—ëŸ¬ í•¸ë“¤ë§ ì¼ê´€ì„± ë¶€ì¡±

**ì˜í–¥ë°›ëŠ” íŒŒì¼**:
- `src/trainer.py` - try-except ê±°ì˜ ì—†ìŒ
- `src/model_agcrn.py` - ì…ë ¥ ê²€ì¦ ì—†ìŒ
- `src/losses.py` - ì¼ë¶€ë§Œ ê²€ì¦

**ì„¸ë¶€ ì‚¬í•­ì€ ìœ„ "6. ì—ëŸ¬ í•¸ë“¤ë§ ë¶€ì¬" ì°¸ê³ **

---

### 2. ì…ë ¥ ê²€ì¦ ë¶€ì¡±

#### ëª¨ë¸ ì…ë ¥ ê²€ì¦

**ìœ„ì¹˜**: `src/model_agcrn.py:119-146`

**í˜„ì¬ ì½”ë“œ**:
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    batch_size = x.shape[0]
    sequence_length = x.shape[1]
    # ì…ë ¥ shape ê²€ì¦ ì—†ìŒ - ì˜ëª»ëœ ì…ë ¥ ì‹œ cryptic error
```

**ê°œì„  ë°©ì•ˆ**:
```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    Forward pass with input validation

    Args:
        x: Input tensor (batch, sequence_length, num_nodes, input_dim)

    Returns:
        Output tensor (batch, num_nodes, output_dim)

    Raises:
        ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    # Shape ê²€ì¦
    if x.ndim != 4:
        raise ValueError(
            f"Expected 4D input (batch, seq, nodes, features), got {x.ndim}D"
        )

    batch_size, sequence_length, num_nodes, input_dim = x.shape

    # ë…¸ë“œ ìˆ˜ ê²€ì¦
    if num_nodes != self.num_nodes:
        raise ValueError(
            f"Expected {self.num_nodes} nodes, got {num_nodes}"
        )

    # íŠ¹ì„± ìˆ˜ ê²€ì¦
    if input_dim != self.input_dim:
        raise ValueError(
            f"Expected {self.input_dim} features, got {input_dim}"
        )

    # NaN/Inf ê²€ì¦
    if torch.isnan(x).any():
        raise ValueError("Input contains NaN values")

    if torch.isinf(x).any():
        raise ValueError("Input contains Inf values")

    # ... ë‚˜ë¨¸ì§€ forward ë¡œì§ ...
```

---

#### Loss í•¨ìˆ˜ ê²€ì¦

**ìœ„ì¹˜**: `src/losses.py`

**ê°œì„  ë°©ì•ˆ**:
```python
class MaskedMSELoss(nn.Module):
    """MSE loss with mask support and validation"""

    def forward(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute masked MSE loss with input validation

        Args:
            pred: Predictions
            target: Ground truth
            mask: Optional mask (True = observed, False = imputed)

        Returns:
            Scalar loss

        Raises:
            ValueError: ì…ë ¥ shape ë¶ˆì¼ì¹˜
        """
        # Shape ê²€ì¦
        if pred.shape != target.shape:
            raise ValueError(
                f"Shape mismatch: pred {pred.shape} vs target {target.shape}"
            )

        if mask is not None and mask.shape != pred.shape:
            raise ValueError(
                f"Mask shape {mask.shape} must match pred shape {pred.shape}"
            )

        # ... ë‚˜ë¨¸ì§€ ë¡œì§ ...
```

---

### 3. ë¡œê¹… ì¼ê´€ì„± ë¶€ì¡±

**í˜„ì¬ ìƒíƒœ**:
- `src/preprocess.py`: logging ëª¨ë“ˆ ì‚¬ìš© âœ“
- `src/trainer.py`: printë§Œ ì‚¬ìš©
- `src/eval.py`: printë§Œ ì‚¬ìš©
- ì¼ê´€ì„± ì—†ìŒ

**ê°œì„  ë°©ì•ˆ**:

**1. ëª¨ë“  íŒŒì¼ì— logging ì ìš©**:

```python
# src/trainer.py
import logging

logger = logging.getLogger(__name__)

class Trainer:
    def __init__(self, model, ...):
        # print ëŒ€ì‹  logger ì‚¬ìš©
        logger.info(f"Initializing trainer on device: {self.device}")
        logger.info(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    def train_epoch(self) -> float:
        logger.debug("Starting training epoch")
        # ...

    def train(self, num_epochs: int, ...) -> Dict:
        logger.info(f"Starting training for {num_epochs} epochs")
        # ...
        logger.info(f"âœ“ Best validation loss: {self.best_val_loss:.6f}")
```

**2. ì¤‘ì•™í™”ëœ ë¡œê¹… ì„¤ì •**:

```python
# src/utils/logger.py
import logging
from pathlib import Path

def setup_logging(log_file: Path, level: str = "INFO"):
    """ì¤‘ì•™í™”ëœ ë¡œê¹… ì„¤ì •"""

    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_file.parent.mkdir(parents=True, exist_ok=True)

    # ë¡œê·¸ ë ˆë²¨
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f"Invalid log level: {level}")

    # í¬ë§¤í„°
    formatter = logging.Formatter(
        '[%(asctime)s] %(name)s - %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(numeric_level)
    file_handler.setFormatter(formatter)

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(formatter)

    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(numeric_level)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)

    return root_logger

# train.pyì—ì„œ ì‚¬ìš©
from src.utils.logger import setup_logging
from src.config import LOG_FILE, LOG_LEVEL

logger = setup_logging(LOG_FILE, LOG_LEVEL)
```

---

### 4. íƒ€ì… ì²´í‚¹ ì¼ê´€ì„± ë¶€ì¡±

**í˜„ì¬ ìƒíƒœ**:
- `src/preprocess.py`: ì™„ì „í•œ íƒ€ì… íŒíŠ¸ âœ“
- `src/dataset.py`: ì™„ì „í•œ íƒ€ì… íŒíŠ¸ âœ“
- `src/trainer.py`: ë¶€ë¶„ì 
- `src/losses.py`: ë¶€ë¶„ì 
- `src/model_agcrn.py`: ë¶€ë¶„ì 
- `src/eval.py`: ë¶€ë¶„ì 

**ê°œì„  ë°©ì•ˆ**:

**1. ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì¶”ê°€**:

```python
# src/trainer.py
from typing import Dict, Optional, List, Any, Tuple
from pathlib import Path
import torch
import torch.nn as nn

class Trainer:
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        device: str = "cpu"
    ) -> None:
        ...

    def train_epoch(self) -> float:
        ...

    def validate(self) -> float:
        ...

    def train(
        self,
        num_epochs: int,
        save_path: Optional[Path] = None
    ) -> Dict[str, List[float]]:
        ...
```

**2. mypy ì„¤ì • ë° CI í†µí•©**:

```ini
# mypy.ini
[mypy]
python_version = 3.8
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True
disallow_incomplete_defs = True
check_untyped_defs = True
no_implicit_optional = True

[mypy-numpy.*]
ignore_missing_imports = True

[mypy-pandas.*]
ignore_missing_imports = True

[mypy-matplotlib.*]
ignore_missing_imports = True
```

```bash
# íƒ€ì… ì²´í¬ ì‹¤í–‰
mypy src/ --strict

# CIì— ì¶”ê°€ (.github/workflows/ci.yml)
- name: Type Check
  run: mypy src/
```

---

### 5. ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ìœ„í—˜

**í˜„ì¬ ìƒíƒœ**: ëŒ€ì²´ë¡œ ì–‘í˜¸ (with ë¬¸ ì‚¬ìš©)

**ê°œì„  í•„ìš” ì˜ì—­**:

```python
# src/trainer.py:216-217
# í˜„ì¬ (Good - with ë¬¸ ì‚¬ìš©)
with open(history_path, 'w') as f:
    json.dump(history, f, indent=2)

# ê°œì„  (ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
try:
    history_path.parent.mkdir(parents=True, exist_ok=True)
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
except IOError as e:
    logger.error(f"Failed to save training history: {e}")
    # ê³„ì† ì§„í–‰ (criticalí•˜ì§€ ì•ŠìŒ)
```

---

### 6. í•˜ë“œì½”ë”©ëœ ê°’ë“¤

#### Magic Numbers

**ìœ„ì¹˜**: `src/trainer.py:99`

**í˜„ì¬ ì½”ë“œ**:
```python
torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)
# 5.0ì´ í•˜ë“œì½”ë”©ë¨
```

**ê°œì„  ë°©ì•ˆ**:
```python
# config.pyì— ì¶”ê°€
GRADIENT_CLIP_NORM = 5.0
GRADIENT_CLIP_TYPE = 'norm'  # 'norm' or 'value'

# trainer.py
from src.config import GRADIENT_CLIP_NORM

if GRADIENT_CLIP_TYPE == 'norm':
    torch.nn.utils.clip_grad_norm_(
        self.model.parameters(),
        max_norm=GRADIENT_CLIP_NORM
    )
```

#### ë¶„ì‚°ëœ ì„¤ì •ê°’

**ê°œì„  ë°©ì•ˆ**:
```python
# config.py - ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì§‘ì¤‘ ê´€ë¦¬

# === Training ===
BATCH_SIZE = int(os.getenv('AGCRN_BATCH_SIZE', '64'))
LEARNING_RATE = float(os.getenv('AGCRN_LEARNING_RATE', '0.001'))
NUM_EPOCHS = int(os.getenv('AGCRN_NUM_EPOCHS', '100'))
GRADIENT_CLIP_NORM = float(os.getenv('AGCRN_GRAD_CLIP', '5.0'))

# === DataLoader ===
NUM_WORKERS = int(os.getenv('AGCRN_NUM_WORKERS', '0'))
PIN_MEMORY = os.getenv('AGCRN_PIN_MEMORY', 'True').lower() == 'true'
PREFETCH_FACTOR = int(os.getenv('AGCRN_PREFETCH_FACTOR', '2'))

# === Dataset ===
MAX_MISSING_GAP = int(os.getenv('AGCRN_MAX_MISSING_GAP', '60'))
STRIDE = int(os.getenv('AGCRN_STRIDE', '1'))
FILTER_LONG_GAPS = os.getenv('AGCRN_FILTER_GAPS', 'True').lower() == 'true'

# === Evaluation ===
NUM_NODES_TO_PLOT = int(os.getenv('AGCRN_PLOT_NODES', '5'))
NUM_SAMPLES_TO_PLOT = int(os.getenv('AGCRN_PLOT_SAMPLES', '100'))
PLOT_DPI = int(os.getenv('AGCRN_PLOT_DPI', '100'))

# === Model ===
HIDDEN_DIM = int(os.getenv('AGCRN_HIDDEN_DIM', '64'))
NUM_LAYERS = int(os.getenv('AGCRN_NUM_LAYERS', '2'))
CHEB_K = int(os.getenv('AGCRN_CHEB_K', '2'))
EMBED_DIM = int(os.getenv('AGCRN_EMBED_DIM', '10'))
```

---

### 7. ê¸°íƒ€ ì½”ë“œ í’ˆì§ˆ ì´ìŠˆ

#### eval.pyì˜ ë§ˆìŠ¤í¬ ë¯¸ì²˜ë¦¬

**ìœ„ì¹˜**: `src/eval.py:40`

**í˜„ì¬ ì½”ë“œ**:
```python
def evaluate_model(model, test_loader, ...):
    for x, y in test_loader:  # ë§ˆìŠ¤í¬ë¥¼ ë°›ì§€ ì•ŠìŒ
        # dataset.pyëŠ” (x, y, masks) ë°˜í™˜í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” ë¬´ì‹œ
```

**ê°œì„  ë°©ì•ˆ**:
```python
def evaluate_model(
    model: nn.Module,
    test_loader: DataLoader,
    criterion: nn.Module,
    device: str = DEVICE,
    use_masks: bool = True
) -> Tuple[float, np.ndarray, np.ndarray]:
    """
    ëª¨ë¸ í‰ê°€ (ë§ˆìŠ¤í¬ ì§€ì›)

    Args:
        model: í‰ê°€í•  ëª¨ë¸
        test_loader: í…ŒìŠ¤íŠ¸ DataLoader
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        use_masks: ë§ˆìŠ¤í¬ ì‚¬ìš© ì—¬ë¶€

    Returns:
        (í‰ê·  ì†ì‹¤, ì˜ˆì¸¡ê°’, ì‹¤ì œê°’)
    """
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for batch_data in test_loader:
            # ë§ˆìŠ¤í¬ ì²˜ë¦¬
            if use_masks and len(batch_data) == 3:
                x, y, masks = batch_data
                mask_target = masks[1][:, -1, :, :] if masks else None
            else:
                x, y = batch_data[0], batch_data[1]
                mask_target = None

            x = x.to(device)
            y = y.to(device)

            output = model(x)
            y_target = y[:, -1, :, :]

            # ì†ì‹¤ ê³„ì‚° (ë§ˆìŠ¤í¬ ì§€ì› ì—¬ë¶€ì— ë”°ë¼)
            if mask_target is not None and hasattr(criterion, 'forward'):
                params = criterion.forward.__code__.co_varnames
                if 'mask' in params:
                    loss = criterion(output, y_target, mask_target.to(device))
                else:
                    loss = criterion(output, y_target)
            else:
                loss = criterion(output, y_target)

            total_loss += loss.item()
            all_predictions.append(output.cpu().numpy())
            all_targets.append(y_target.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)

    return avg_loss, predictions, targets
```

---

#### ë³µì¡í•œ ë™ì  ê²€ì‚¬ ë¡œì§

**ìœ„ì¹˜**: `src/trainer.py:82-95`

**í˜„ì¬ ì½”ë“œ**:
```python
# ë³µì¡í•˜ê³  ì½ê¸° ì–´ë ¤ìš´ ë™ì  ê²€ì‚¬
if hasattr(self.criterion, 'forward') and \
   'mask' in self.criterion.forward.__code__.co_varnames:
    loss = self.criterion(output, y_target, mask_target)
else:
    loss = self.criterion(output, y_target)
```

**ê°œì„  ë°©ì•ˆ 1: ABC ì‚¬ìš©**:
```python
from abc import ABC, abstractmethod

class Loss(ABC):
    """ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        pass

class MaskedLoss(ABC):
    """ë§ˆìŠ¤í¬ ì§€ì› ì†ì‹¤ í•¨ìˆ˜ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def forward(
        self,
        pred: torch.Tensor,
        target: torch.Tensor,
        mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        pass

# Trainerì—ì„œ
class Trainer:
    def __init__(self, ..., criterion_uses_mask: bool = False):
        self.criterion_uses_mask = criterion_uses_mask

    def train_epoch(self):
        if self.criterion_uses_mask:
            loss = self.criterion(output, y_target, mask_target)
        else:
            loss = self.criterion(output, y_target)
```

**ê°œì„  ë°©ì•ˆ 2: Duck typing with try-except**:
```python
def compute_loss(
    self,
    output: torch.Tensor,
    target: torch.Tensor,
    mask: Optional[torch.Tensor] = None
) -> torch.Tensor:
    """ì†ì‹¤ ê³„ì‚° (ë§ˆìŠ¤í¬ ìë™ ê°ì§€)"""
    try:
        # ë§ˆìŠ¤í¬ ì§€ì› ì‹œë„
        return self.criterion(output, target, mask)
    except TypeError:
        # ë§ˆìŠ¤í¬ ë¯¸ì§€ì› ì‹œ fallback
        return self.criterion(output, target)
```

---

## ì„¤ì • ë° í™˜ê²½ ë¬¸ì œ

### 1. í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬

ìœ„ì˜ "7. í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬ ë¶€ì¬" ì°¸ê³ 

---

### 2. ì„¤ì • íŒŒì¼ ë³´ì•ˆ

**í˜„ì¬ ìƒíƒœ**: ëª¨ë“  ì„¤ì •ì´ ì½”ë“œì— í•˜ë“œì½”ë”©
**ìœ„í—˜ë„**: LOW

**ê°œì„  ë°©ì•ˆ**:

**1. YAML ê¸°ë°˜ ì„¤ì • íŒŒì¼**:

```yaml
# configs/default.yaml
environment: development

training:
  batch_size: 64
  learning_rate: 0.001
  num_epochs: 100
  gradient_clip_norm: 5.0
  device: cuda

model:
  hidden_dim: 64
  num_layers: 2
  cheb_k: 2
  embed_dim: 10
  output_dim: 1

data:
  node_mode: raw_id
  features:
    - flow
    - occupancy
    - harmonicMeanSpeed
  sequence_length: 12
  horizon: 3

dataset:
  max_missing_gap: 60
  stride: 1
  filter_long_gaps: true

logging:
  level: INFO
  file: logs/agcrn.log
```

**2. ì„¤ì • ë¡œë”**:

```python
# src/utils/config_loader.py
import yaml
from pathlib import Path
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

def load_config(config_path: Path) -> Dict[str, Any]:
    """
    YAML ì„¤ì • íŒŒì¼ ë¡œë“œ

    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ

    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Raises:
        FileNotFoundError: ì„¤ì • íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°
        yaml.YAMLError: YAML íŒŒì‹± ì‹¤íŒ¨
    """
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)  # safe_load ì‚¬ìš©!

        logger.info(f"Loaded config from {config_path}")
        return config

    except yaml.YAMLError as e:
        logger.error(f"Failed to parse YAML config: {e}")
        raise

def merge_configs(
    default_config: Dict[str, Any],
    override_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë³‘í•© (overrideê°€ ìš°ì„ )

    Args:
        default_config: ê¸°ë³¸ ì„¤ì •
        override_config: ì˜¤ë²„ë¼ì´ë“œ ì„¤ì •

    Returns:
        ë³‘í•©ëœ ì„¤ì •
    """
    merged = default_config.copy()

    for key, value in override_config.items():
        if isinstance(value, dict) and key in merged and isinstance(merged[key], dict):
            merged[key] = merge_configs(merged[key], value)
        else:
            merged[key] = value

    return merged

# ì‚¬ìš© ì˜ˆì‹œ
def get_config(config_name: str = 'default') -> Dict[str, Any]:
    """ì„¤ì • ë¡œë“œ (í™˜ê²½ë³„ ì˜¤ë²„ë¼ì´ë“œ ì§€ì›)"""
    config_dir = Path(__file__).parent.parent.parent / 'configs'

    # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
    default_config = load_config(config_dir / 'default.yaml')

    # í™˜ê²½ë³„ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    env_config_path = config_dir / f'{config_name}.yaml'
    if env_config_path.exists():
        env_config = load_config(env_config_path)
        config = merge_configs(default_config, env_config)
    else:
        config = default_config

    return config
```

**3. train.pyì—ì„œ ì‚¬ìš©**:

```python
from src.utils.config_loader import get_config

# ì„¤ì • ë¡œë“œ
config = get_config(args.config)  # --config development/production

# ì„¤ì • ì‚¬ìš©
batch_size = config['training']['batch_size']
learning_rate = config['training']['learning_rate']
```

---

## ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ê¶Œì¥ì‚¬í•­

### ğŸ”´ ì¦‰ì‹œ ìˆ˜ì • í•„ìš” (HIGH Priority)

#### 1. Pickle Deserialization ì·¨ì•½ì  ìˆ˜ì •
**íŒŒì¼**: `src/dataset.py:158`
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 5ë¶„
**ë‚œì´ë„**: ì‰¬ì›€

```python
# ë³€ê²½ ì „
data = np.load(npz_path, allow_pickle=True)

# ë³€ê²½ í›„
data = np.load(npz_path, allow_pickle=False)
```

#### 2. Unsafe PyTorch Model Loading ìˆ˜ì •
**íŒŒì¼**: `src/eval.py:145`
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 10ë¶„
**ë‚œì´ë„**: ì‰¬ì›€

```python
# ë³€ê²½ ì „
checkpoint = torch.load(model_path, map_location='cpu')

# ë³€ê²½ í›„
try:
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
except TypeError:
    # PyTorch 1.x í˜¸í™˜ì„±
    import warnings
    warnings.warn("Using unsafe torch.load - upgrade to PyTorch 2.0+")
    checkpoint = torch.load(model_path, map_location='cpu')
```

#### 3. Path Traversal ë°©ì§€
**íŒŒì¼**: ì—¬ëŸ¬ íŒŒì¼
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 2ì‹œê°„
**ë‚œì´ë„**: ì¤‘ê°„

- `validate_file_path()` í•¨ìˆ˜ êµ¬í˜„
- ëª¨ë“  íŒŒì¼ I/Oì— ê²€ì¦ ì¶”ê°€
- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±

---

### ğŸŸ¡ ë‹¨ê¸° ê°œì„  (MEDIUM Priority, 1-2ì£¼)

#### 4. ì…ë ¥ ê²€ì¦ ì¶”ê°€
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 4ì‹œê°„
**ë‚œì´ë„**: ì¤‘ê°„

- [ ] íŒŒì¼ í¬ê¸° ê²€ì¦ (`load_csv_data()`)
- [ ] ëª…ë ¹í–‰ ì¸ì ê²€ì¦ (`validate_args()`)
- [ ] ëª¨ë¸ ì…ë ¥ ê²€ì¦ (shape, NaN/Inf ì²´í¬)
- [ ] Loss í•¨ìˆ˜ ì…ë ¥ ê²€ì¦

#### 5. ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„ 
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 6ì‹œê°„
**ë‚œì´ë„**: ì¤‘ê°„

- [ ] Trainerì— try-except ì¶”ê°€
- [ ] GPU OOM ì²˜ë¦¬
- [ ] NaN loss ì²˜ë¦¬
- [ ] ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨ ì²˜ë¦¬
- [ ] KeyboardInterrupt ì²˜ë¦¬

#### 6. ì˜ì¡´ì„± ë²„ì „ ê³ ì • ë° ë³´ì•ˆ ìŠ¤ìº”
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 2ì‹œê°„
**ë‚œì´ë„**: ì‰¬ì›€

- [ ] requirements.txt ë²„ì „ ê³ ì •
- [ ] safety ì„¤ì¹˜ ë° ìŠ¤ìº”
- [ ] bandit ì„¤ì¹˜ ë° ìŠ¤ìº”
- [ ] GitHub Actions ë³´ì•ˆ ì›Œí¬í”Œë¡œìš° ì¶”ê°€

---

### ğŸ”µ ì¥ê¸° ê°œì„  (LOW Priority, 1-2ê°œì›”)

#### 7. ë¡œê¹… í†µì¼
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 4ì‹œê°„
**ë‚œì´ë„**: ì‰¬ì›€

- [ ] ëª¨ë“  íŒŒì¼ì— logging ëª¨ë“ˆ ì ìš©
- [ ] print ë¬¸ ì œê±°
- [ ] ì¤‘ì•™í™”ëœ ë¡œê¹… ì„¤ì • (`setup_logging()`)
- [ ] ë¡œê·¸ ë ˆë²¨ í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´

#### 8. íƒ€ì… íŒíŠ¸ ì™„ì„±
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 6ì‹œê°„
**ë‚œì´ë„**: ì¤‘ê°„

- [ ] ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì¶”ê°€
- [ ] mypy ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] CIì— íƒ€ì… ì²´í¬ ì¶”ê°€
- [ ] íƒ€ì… ì—ëŸ¬ ìˆ˜ì •

#### 9. í™˜ê²½ë³€ìˆ˜ ì§€ì›
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 3ì‹œê°„
**ë‚œì´ë„**: ì‰¬ì›€

- [ ] python-dotenv ì„¤ì¹˜
- [ ] .env.example ì‘ì„±
- [ ] config.pyì— í™˜ê²½ë³€ìˆ˜ ì§€ì› ì¶”ê°€
- [ ] .gitignoreì— .env ì¶”ê°€

#### 10. ì„¤ì • íŒŒì¼ ì™¸ë¶€í™”
**ì˜ˆìƒ ì‘ì—… ì‹œê°„**: 4ì‹œê°„
**ë‚œì´ë„**: ì¤‘ê°„

- [ ] YAML ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] config_loader.py êµ¬í˜„
- [ ] í™˜ê²½ë³„ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
- [ ] train.pyì— --config ì˜µì…˜ ì¶”ê°€

---

## ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” (HIGH)

- [ ] **Pickle deserialization ì·¨ì•½ì  ìˆ˜ì •**
  - [ ] src/dataset.py:158 - `allow_pickle=False`
  - [ ] ë°ì´í„° íŒŒì¼ ì¬ìƒì„± (.npz íŒŒì¼ì´ pickle ê°ì²´ í¬í•¨ ì‹œ)

- [ ] **PyTorch model loading ë³´ì•ˆ**
  - [ ] src/eval.py:145 - `weights_only=True` ì¶”ê°€
  - [ ] src/trainer.py ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ë„ í™•ì¸

- [ ] **Path traversal ë°©ì§€**
  - [ ] `validate_file_path()` í•¨ìˆ˜ êµ¬í˜„
  - [ ] src/preprocess.py íŒŒì¼ I/O ê²€ì¦ ì¶”ê°€
  - [ ] src/trainer.py íŒŒì¼ I/O ê²€ì¦ ì¶”ê°€
  - [ ] src/eval.py íŒŒì¼ I/O ê²€ì¦ ì¶”ê°€

### ë‹¨ê¸° ê°œì„  (MEDIUM)

- [ ] **ì…ë ¥ ê²€ì¦**
  - [ ] íŒŒì¼ í¬ê¸° ì œí•œ (`load_csv_data()`)
  - [ ] ëª…ë ¹í–‰ ì¸ì ë²”ìœ„ ê²€ì¦ (`validate_args()`)
  - [ ] ëª¨ë¸ ì…ë ¥ ê²€ì¦ (shape, NaN/Inf)

- [ ] **ì—ëŸ¬ í•¸ë“¤ë§**
  - [ ] Trainer.train_epoch() try-except ì¶”ê°€
  - [ ] GPU OOM ì²˜ë¦¬
  - [ ] NaN loss ì²˜ë¦¬
  - [ ] ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨ ì²˜ë¦¬

- [ ] **ì˜ì¡´ì„± ë³´ì•ˆ**
  - [ ] requirements.txt ë²„ì „ ê³ ì •
  - [ ] `pip install safety && safety check` ì‹¤í–‰
  - [ ] `pip install bandit && bandit -r src/` ì‹¤í–‰
  - [ ] GitHub Actionsì— ë³´ì•ˆ ìŠ¤ìº” ì¶”ê°€

### ì¥ê¸° ê°œì„  (LOW)

- [ ] **ë¡œê¹…**
  - [ ] ëª¨ë“  printë¥¼ loggerë¡œ ë³€ê²½
  - [ ] setup_logging() êµ¬í˜„
  - [ ] ë¡œê·¸ ë ˆë²¨ í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´

- [ ] **íƒ€ì… ì²´í‚¹**
  - [ ] ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì¶”ê°€
  - [ ] mypy.ini ì‘ì„±
  - [ ] CIì— mypy ì¶”ê°€

- [ ] **í™˜ê²½ë³€ìˆ˜**
  - [ ] .env.example ì‘ì„±
  - [ ] config.pyì— os.getenv() ì¶”ê°€
  - [ ] .gitignoreì— .env ì¶”ê°€

- [ ] **ì„¤ì • ì™¸ë¶€í™”**
  - [ ] configs/default.yaml ì‘ì„±
  - [ ] config_loader.py êµ¬í˜„
  - [ ] train.pyì— --config ì˜µì…˜ ì¶”ê°€

### ë³´ì•ˆ ëª¨ë‹ˆí„°ë§

- [ ] **ì •ê¸°ì ì¸ ë³´ì•ˆ ìŠ¤ìº”**
  - [ ] ë§¤ì£¼ `safety check` ì‹¤í–‰
  - [ ] ë§¤ì›” ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ ê²€í† 
  - [ ] GitHub Dependabot í™œì„±í™”

- [ ] **ì½”ë“œ ë¦¬ë·°**
  - [ ] PRì— ë³´ì•ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
  - [ ] íŒŒì¼ I/O ì½”ë“œ ì§‘ì¤‘ ë¦¬ë·°
  - [ ] ë¯¼ê°í•œ ì •ë³´ í•˜ë“œì½”ë”© ë°©ì§€

---

## ì°¸ê³  ìë£Œ

### ë³´ì•ˆ ê°€ì´ë“œë¼ì¸

- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [CWE Top 25](https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html)
- [Python Security Best Practices](https://python.readthedocs.io/en/stable/library/security_warnings.html)

### ë„êµ¬

- [Safety](https://pyup.io/safety/) - Python dependency vulnerability scanner
- [Bandit](https://bandit.readthedocs.io/) - Python code security scanner
- [mypy](https://mypy.readthedocs.io/) - Static type checker
- [pre-commit](https://pre-commit.com/) - Git hook framework

### ì·¨ì•½ì  ë°ì´í„°ë² ì´ìŠ¤

- [NVD (National Vulnerability Database)](https://nvd.nist.gov/)
- [CVE Details](https://www.cvedetails.com/)
- [GitHub Advisory Database](https://github.com/advisories)

---

## ìš”ì•½

### ì£¼ìš” ë°œê²¬ ì‚¬í•­

1. **2ê°œì˜ HIGH ìœ„í—˜ ì·¨ì•½ì ** (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)
   - Pickle deserialization (RCE ê°€ëŠ¥)
   - Unsafe model loading (RCE ê°€ëŠ¥)

2. **4ê°œì˜ MEDIUM ìœ„í—˜ ì´ìŠˆ** (ë‹¨ê¸° ê°œì„ )
   - Path traversal
   - ì…ë ¥ ê²€ì¦ ë¶€ì¬
   - ì˜ì¡´ì„± ì·¨ì•½ì 
   - ì—ëŸ¬ í•¸ë“¤ë§ ë¶€ì¡±

3. **6ê°œì˜ LOW ìœ„í—˜ ì´ìŠˆ** (ì¥ê¸° ê°œì„ )
   - í™˜ê²½ë³€ìˆ˜ ê´€ë¦¬
   - ë¡œê¹… ì¼ê´€ì„±
   - íƒ€ì… ì²´í‚¹
   - ì„¤ì • ì™¸ë¶€í™” ë“±

### ê¶Œì¥ ì¡°ì¹˜ ìˆœì„œ

1. **1ì¼ì°¨**: HIGH ì·¨ì•½ì  ì¦‰ì‹œ ìˆ˜ì • (Pickle, Model loading)
2. **1ì£¼ì°¨**: Path traversal ë°©ì§€ + ì…ë ¥ ê²€ì¦
3. **2ì£¼ì°¨**: ì—ëŸ¬ í•¸ë“¤ë§ + ì˜ì¡´ì„± ê³ ì •
4. **1ê°œì›”ì°¨**: ë¡œê¹…/íƒ€ì… ì²´í‚¹/í™˜ê²½ë³€ìˆ˜ ì§€ì›
5. **ì§€ì†ì **: ë³´ì•ˆ ìŠ¤ìº” ìë™í™” + ëª¨ë‹ˆí„°ë§

### ì˜ˆìƒ ê°œì„  íš¨ê³¼

- **ë³´ì•ˆ**: RCE ì·¨ì•½ì  ì œê±°, ì…ë ¥ ê²€ì¦ìœ¼ë¡œ DoS ë°©ì§€
- **ì•ˆì •ì„±**: ì—ëŸ¬ í•¸ë“¤ë§ìœ¼ë¡œ í¬ë˜ì‹œ ê°ì†Œ
- **ìœ ì§€ë³´ìˆ˜ì„±**: ë¡œê¹…/íƒ€ì… ì²´í‚¹ìœ¼ë¡œ ë””ë²„ê¹… ìš©ì´
- **ë°°í¬**: í™˜ê²½ë³€ìˆ˜ë¡œ ë‹¤ì–‘í•œ í™˜ê²½ ì§€ì›

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-17
**ì‘ì„±ì**: Claude Code Security Review
