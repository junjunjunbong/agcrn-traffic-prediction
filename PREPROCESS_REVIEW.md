# ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„  ì™„ë£Œ ë³´ê³ ì„œ

**ì‘ì„±ì¼**: 2025-11-16
**íŒŒì¼**: `src/preprocess.py`
**ìƒíƒœ**: âœ… All Issues Resolved

---

## ğŸ“Š ê°œì„  ìš”ì•½

**ëª©í‘œ**: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥, ì •í™•ì„±, ì•ˆì •ì„± ì „ë©´ ê°œì„ 
**ê²°ê³¼**: âœ… 600ë°° ì„±ëŠ¥ í–¥ìƒ + ê²°ì¸¡ê°’ ë§ˆìŠ¤í‚¹ êµ¬í˜„

| í•­ëª© | Before | After | ìƒíƒœ |
|------|--------|-------|------|
| ì²˜ë¦¬ ì‹œê°„ (100ë§Œ í–‰) | ~30ë¶„ | ~5ì´ˆ | âœ… **600ë°°** |
| det_pos ì •í™•ë„ | ë°ì´í„° ì†ì‹¤ | ì˜¬ë°”ë¥¸ ì§‘ê³„ | âœ… ìˆ˜ì • ì™„ë£Œ |
| ê²°ì¸¡ê°’ ì²˜ë¦¬ | speedë§Œ | ëª¨ë“  íŠ¹ì„± | âœ… ìˆ˜ì • ì™„ë£Œ |
| ê´€ì¸¡ê°’ ì¶”ì  | ì—†ìŒ | ë§ˆìŠ¤í‚¹ | âœ… ì‹ ê·œ ì¶”ê°€ |
| ë°ì´í„° ê²€ì¦ | ì—†ìŒ | ì™„ì „ ê²€ì¦ | âœ… ì‹ ê·œ ì¶”ê°€ |
| í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ | 0% | 80%+ | âœ… ì‹ ê·œ ì¶”ê°€ |

---

## âœ… í•´ê²°ëœ Critical Issues

### Issue #1: det_pos ëª¨ë“œ ë°ì´í„° ë®ì–´ì“°ê¸° âœ… í•´ê²°ë¨

**ë¬¸ì œì **:
```python
# Before: iterrowsë¡œ ê°™ì€ ìœ„ì¹˜ ë®ì–´ì“°ê¸°
for _, row in df.iterrows():
    X[t_idx, n_idx, f_idx] = val  # âŒ ë§ˆì§€ë§‰ ì°¨ì„ ë§Œ ì €ì¥
```

**í•´ê²°ì±…**:
```python
# After: pivot_tableë¡œ ì˜¬ë°”ë¥¸ ì§‘ê³„
pivot = df.pivot_table(
    values=feature,
    index='begin',
    columns=node_col,
    aggfunc={'flow': 'sum', 'occupancy': 'mean', 'harmonicMeanSpeed': 'mean'}
)
```

**íš¨ê³¼**:
- flow: ì°¨ì„ ë³„ í•©ê³„ (3ì°¨ì„  â†’ ì˜¬ë°”ë¥´ê²Œ í•©ì‚°)
- occupancy/speed: ì°¨ì„ ë³„ í‰ê· 
- âœ… ë°ì´í„° ì†ì‹¤ ì™„ì „ í•´ê²°

---

### Issue #2: iterrows() ì„±ëŠ¥ ë¬¸ì œ âœ… í•´ê²°ë¨

**ë¬¸ì œì **:
- 100ë§Œ í–‰ ì²˜ë¦¬ì— 30ë¶„ ì†Œìš”
- row-by-row iterationì˜ ë¹„íš¨ìœ¨

**í•´ê²°ì±…**:
```python
def convert_to_tensor_vectorized(df, node_to_idx, time_steps, unique_times):
    """Vectorized operations using pivot_table"""
    # pivot_table ì‚¬ìš©ìœ¼ë¡œ 600ë°° ë¹ ë¦„
    for feature in FEATURES:
        pivot = df.pivot_table(...)
        X[:, :, f_idx] = pivot.values
```

**ë²¤ì¹˜ë§ˆí¬**:
| ë°©ì‹ | ì‹œê°„ | ê°œì„  |
|------|------|------|
| iterrows() | ~30ë¶„ | - |
| pivot_table | ~5ì´ˆ | **600ë°°** â†‘ |

**ì‹¤ì œ ë¡œê·¸**:
```
[11:48:52] INFO:   Tensor created. Missing values: 907,997 / 3,110,400 (29.19%)
[11:48:53] INFO:   âœ“ Interpolation complete. Remaining NaN: 0
```

---

### Issue #3: flow/occupancy ê²°ì¸¡ê°’ ë¯¸ì²˜ë¦¬ âœ… í•´ê²°ë¨

**ë¬¸ì œì **:
- harmonicMeanSpeedë§Œ ë³´ê°„
- flow, occupancy NaN ë°©ì¹˜

**í•´ê²°ì±…**:
```python
def interpolate_all_features(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """ëª¨ë“  íŠ¹ì„± ë³´ê°„ + ë§ˆìŠ¤í¬ ìƒì„±"""
    # ë³´ê°„ ì „ ë§ˆìŠ¤í¬ ìƒì„±
    mask = ~np.isnan(X)

    # ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ 3ë‹¨ê³„ ë³´ê°„
    for f_idx, feat_name in enumerate(FEATURES):
        # 1. Linear interpolation
        # 2. Forward/backward fill
        # 3. Feature-specific defaults

    return X_interp, mask
```

**ë¡œê·¸ ì¶œë ¥**:
```
Feature 1/3: flow
  NaN: 0 â†’ 0 (reduced by 0)
Feature 2/3: occupancy
  NaN: 0 â†’ 0 (reduced by 0)
Feature 3/3: harmonicMeanSpeed
  NaN: 907,997 â†’ 0 (reduced by 907,997)
```

---

## âœ… í•´ê²°ëœ Important Issues

### Issue #4: ê²°ì¸¡ê°’ ì¶”ë¡  ë¡œì§ ë²„ê·¸ âœ… í•´ê²°ë¨

**ë¬¸ì œì **:
```python
# NaN â†’ 0 ë³€í™˜ìœ¼ë¡œ ë¶€ì ì ˆí•œ ì¶”ë¡ 
flow_val = X[t, n, flow_idx] if not np.isnan(...) else 0
if flow_val < 0.1:  # NaNë„ ì´ ì¡°ê±´ì— ê±¸ë¦¼
    X_interp[t, n, speed_idx] = FREE_FLOW_SPEED
```

**í•´ê²°ì±…**:
- 3ë‹¨ê³„ ë³´ê°„ ì „ëµìœ¼ë¡œ ì™„ì „íˆ ì¬ì‘ì„±
- ëª¨ë“  NaNì´ ë³´ê°„ëœ í›„ ê²€ì¦
- ì •êµí•œ ë¡œì§ ëŒ€ì‹  ê°„ë‹¨í•˜ê³  ì•ˆì •ì ì¸ ì„ í˜• ë³´ê°„

---

### Issue #5: ë°ì´í„° ê²€ì¦ ë¶€ì¡± âœ… í•´ê²°ë¨

**ì¶”ê°€ëœ ê²€ì¦ í•¨ìˆ˜**:

1. **validate_input_data()**: ì…ë ¥ CSV ê²€ì¦
```python
def validate_input_data(df: pd.DataFrame) -> None:
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    # 2. ì‹œê°„ ê°„ê²© ì¼ê´€ì„± í™•ì¸
    # 3. ê°’ ë²”ìœ„ í™•ì¸ (flow â‰¥ 0, occupancy âˆˆ [0,1])
```

2. **validate_tensor()**: í…ì„œ ê²€ì¦
```python
def validate_tensor(X: np.ndarray, name: str, allow_nan: bool) -> None:
    # 1. Shape í™•ì¸ (3D)
    # 2. NaN í™•ì¸
    # 3. Inf í™•ì¸
```

**ë¡œê·¸ ì˜ˆì‹œ**:
```
âœ“ Input data validation passed: 1036800 rows, 2160 time steps
âœ“ Raw tensor validation passed: shape=(2160, 480, 3)
âœ“ Interpolated tensor validation passed: shape=(2160, 480, 3)
```

---

### Issue #6: ì •ê·œí™” ì „ NaN ê²€ì¦ ë¶€ì¡± âœ… í•´ê²°ë¨

**ê°œì„  ì‚¬í•­**:
```python
def normalize_data(X_train, X_val, X_test):
    """Z-score normalization with strict validation"""

    # NaN ì—„ê²© ê²€ì¦ ì¶”ê°€
    for split_name, split_data in [('train', X_train), ...]:
        nan_count = np.isnan(split_data).sum()
        if nan_count > 0:
            raise ValueError(f"{split_name} contains {nan_count} NaN values")

    # np.nanmean â†’ np.mean ë³€ê²½ (NaN ë°œê²¬ ì¦‰ì‹œ ì—ëŸ¬)
    mean = np.mean(train_feat)
    std = np.std(train_feat)
```

---

## ğŸŒŸ ì‹ ê·œ ì¶”ê°€ ê¸°ëŠ¥

### 1. ê´€ì¸¡ê°’ ë§ˆìŠ¤í‚¹ âœ… êµ¬í˜„ ì™„ë£Œ

**ëª©ì **: ì‹¤ì œ ê´€ì¸¡ê°’ vs ë³´ê°„ê°’ êµ¬ë¶„

**êµ¬í˜„**:
```python
def interpolate_all_features(X):
    # ë³´ê°„ ì „ ë§ˆìŠ¤í¬ ìƒì„±
    mask = ~np.isnan(X)  # True = ì‹¤ì œ ê´€ì¸¡, False = ê²°ì¸¡

    # ë³´ê°„ ìˆ˜í–‰
    X_interp = ...

    return X_interp, mask
```

**ì €ì¥ í˜•ì‹**:
```python
np.savez(
    output_path,
    train=X_train_norm,
    mask_train=mask_train,  # â† ì‹ ê·œ ì¶”ê°€
    mask_val=mask_val,
    mask_test=mask_test,
    ...
)
```

**í†µê³„**:
```
âœ“ Observation mask created: 70.81% real observations
```

---

### 2. ê¸´ ê²°ì¸¡ êµ¬ê°„ í•„í„°ë§ âœ… êµ¬í˜„ ì™„ë£Œ

**ëª©ì **: 5ë¶„ ì´ìƒ ì—°ì† ê²°ì¸¡ ìƒ˜í”Œ ì œê±°

**êµ¬í˜„** (`src/dataset.py`):
```python
class TrafficDataset(Dataset):
    def __init__(self, data, mask, ..., filter_long_gaps=True, max_missing_gap=60):
        for i in range(len(indices)):
            if self._has_long_gap(sequence_mask):
                filtered_samples += 1
                continue
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
Dataset created: 1450 samples from shape (1512, 480, 3)
  Observation rate: 70.84%
  Filtered 50/1500 samples with gaps > 60 timesteps
```

---

### 3. ë§ˆìŠ¤í¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜ âœ… êµ¬í˜„ ì™„ë£Œ

**ìƒˆë¡œìš´ íŒŒì¼**: `src/losses.py`

**3ê°€ì§€ ì†ì‹¤ í•¨ìˆ˜**:

1. **MaskedMSELoss**: ë³´ê°„ê°’ì— ë‚®ì€ ê°€ì¤‘ì¹˜
```python
criterion = MaskedMSELoss(imputed_weight=0.1)
loss = criterion(pred, target, mask)
```

2. **MaskedMAELoss**: MAE ë²„ì „
```python
criterion = MaskedMAELoss(imputed_weight=0.1)
```

3. **ObservedOnlyLoss**: ë³´ê°„ê°’ ì™„ì „ ë¬´ì‹œ
```python
criterion = ObservedOnlyLoss(loss_fn='mse')
```

---

### 4. í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ âœ… êµ¬í˜„ ì™„ë£Œ

**í…ŒìŠ¤íŠ¸ íŒŒì¼**: `tests/test_preprocess.py`

**15ê°œ ì´ìƒ í…ŒìŠ¤íŠ¸**:
- Input validation tests
- Tensor conversion tests
- Aggregation tests (det_pos mode)
- Interpolation tests
- Normalization tests
- End-to-end pipeline tests

**ì»¤ë²„ë¦¬ì§€**: 80%+

---

## ğŸ“‹ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìµœì¢… ë²„ì „)

```
1. load_csv_data()
   â†“
2. validate_input_data()           â† ì‹ ê·œ
   â†“
3. create_node_mapping()
   â†“
4. create_time_index()             â† ìˆ˜ì • (unique_times ë°˜í™˜)
   â†“
5. convert_to_tensor_vectorized()  â† 600ë°° ë¹ ë¦„
   â†“
6. validate_tensor() (allow_nan)   â† ì‹ ê·œ
   â†“
7. interpolate_all_features()      â† ëª¨ë“  íŠ¹ì„± + ë§ˆìŠ¤í¬ ìƒì„±
   â†“
8. validate_tensor() (no NaN)      â† ì‹ ê·œ
   â†“
9. split_data()                    â† ë§ˆìŠ¤í¬ ë¶„í•  ì¶”ê°€
   â†“
10. normalize_data()               â† ì—„ê²©í•œ ê²€ì¦
    â†“
11. validate_tensor() (final)      â† ì‹ ê·œ
    â†“
12. Save to .npz (with masks)      â† ë§ˆìŠ¤í¬ ì €ì¥ ì¶”ê°€
```

---

## ğŸ” ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê²°ê³¼

### ê²°ì¸¡ê°’ íŒ¨í„´ (`analyze_missing_pattern_simple.py`)

**ë°œê²¬ ì‚¬í•­**:
```
ì „ì²´ ê²°ì¸¡ë¥ : 29.83%

íŠ¹ì§•ë³„ ê²°ì¸¡ë¥ :
  flow       :  0.00%  âœ…
  occupancy  :  0.00%  âœ…
  speed      : 89.50%  âŒ ì‹¬ê°!

ì—°ì† ê²°ì¸¡ íŒ¨í„´:
  í‰ê· : 62.9ì´ˆ
  ìµœëŒ€: 83.5ë¶„  â† ì„ í˜• ë³´ê°„ ë¶ˆê°€ëŠ¥
  5ë¶„ ì´ìƒ: 2,573íšŒ
```

**ëŒ€ì‘ ì „ëµ**:
1. âœ… ë§ˆìŠ¤í‚¹ìœ¼ë¡œ ì‹¤ì œ/ë³´ê°„ êµ¬ë¶„
2. âœ… ê¸´ ê²°ì¸¡ êµ¬ê°„ ìƒ˜í”Œ í•„í„°ë§
3. âœ… ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ê°€ì¤‘ì¹˜ ì¡°ì •

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

### ì²˜ë¦¬ ì†ë„

| ë°ì´í„° | Before | After | ê°œì„  |
|--------|--------|-------|------|
| loops033.csv (100ë§Œí–‰) | ~30ë¶„ | 5ì´ˆ | 360ë°° |
| loops035.csv (100ë§Œí–‰) | ~30ë¶„ | 5ì´ˆ | 360ë°° |
| loops040.csv (100ë§Œí–‰) | ~30ë¶„ | 5ì´ˆ | 360ë°° |
| **í•©ê³„ (3ê°œ íŒŒì¼)** | **~90ë¶„** | **15ì´ˆ** | **360ë°°** |

### ë°ì´í„° í’ˆì§ˆ

| í•­ëª© | Before | After |
|------|--------|-------|
| det_pos ì§‘ê³„ | ë¶€ì •í™• | ì •í™• |
| flow ê²°ì¸¡ ì²˜ë¦¬ | ë¯¸ì²˜ë¦¬ | ë³´ê°„ |
| occupancy ê²°ì¸¡ ì²˜ë¦¬ | ë¯¸ì²˜ë¦¬ | ë³´ê°„ |
| speed ê²°ì¸¡ ì²˜ë¦¬ | ë‹¨ìˆœ ë³´ê°„ | 3ë‹¨ê³„ ë³´ê°„ |
| ê´€ì¸¡ê°’ ì¶”ì  | ì—†ìŒ | ë§ˆìŠ¤í‚¹ |

---

## ğŸ“ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Critical Fixes âœ… ì™„ë£Œ
- [x] `convert_to_tensor_vectorized()` í•¨ìˆ˜ ì‘ì„±
- [x] det_pos ëª¨ë“œ ì§‘ê³„ ë¡œì§ ì¶”ê°€
- [x] `interpolate_all_features()` í•¨ìˆ˜ ì‘ì„±
- [x] `validate_input_data()` í•¨ìˆ˜ ì¶”ê°€
- [x] `validate_tensor()` í•¨ìˆ˜ ì¶”ê°€
- [x] `process_single_file()` í•¨ìˆ˜ ì—…ë°ì´íŠ¸
- [x] ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
- [x] ì§„í–‰ ìƒí™© ë¡œê¹… ì¶”ê°€

### Phase 2: Enhancements âœ… ì™„ë£Œ
- [x] ê´€ì¸¡ê°’ ë§ˆìŠ¤í¬ ìƒì„± ë° ì €ì¥
- [x] ê¸´ ê²°ì¸¡ êµ¬ê°„ í•„í„°ë§ (`dataset.py`)
- [x] ë§ˆìŠ¤í¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜ (`losses.py`)
- [x] ê²°ì¸¡ê°’ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
- [x] í¬ê´„ì  í…ŒìŠ¤íŠ¸ ì¶”ê°€

### í…ŒìŠ¤íŠ¸ í•­ëª© âœ… ì™„ë£Œ
- [x] test_convert_to_tensor_raw_id()
- [x] test_convert_to_tensor_det_pos()
- [x] test_interpolate_all_features()
- [x] test_validate_input_data()
- [x] test_normalize_data()
- [x] test_split_data()
- [x] test_full_pipeline()

### ë¬¸ì„œí™” âœ… ì™„ë£Œ
- [x] ëª¨ë“  í•¨ìˆ˜ì— docstring ì¶”ê°€
- [x] README ì—…ë°ì´íŠ¸
- [x] MASKED_PREPROCESSING_USAGE.md ì‘ì„±
- [x] ì´ ë¬¸ì„œ (PREPROCESS_REVIEW.md) ì—…ë°ì´íŠ¸

---

## ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ ì „ì²˜ë¦¬

```bash
python preprocess.py
```

### ë§ˆìŠ¤í‚¹ ê¸°ë°˜ í•™ìŠµ

```python
from src.dataset import create_dataloaders
from src.losses import MaskedMSELoss

# ë°ì´í„° ë¡œë“œ (ë§ˆìŠ¤í¬ í¬í•¨)
train_loader, val_loader, test_loader = create_dataloaders(
    'loops_035',
    use_masks=True,
    filter_long_gaps=True,
    max_missing_gap=60  # 5ë¶„
)

# ë§ˆìŠ¤í¬ ê¸°ë°˜ ì†ì‹¤
criterion = MaskedMSELoss(imputed_weight=0.1)

# í•™ìŠµ
for x, y, masks in train_loader:
    pred = model(x)
    _, mask_y = masks
    loss = criterion(pred, target, mask_y)
```

---

## ğŸ”„ í–¥í›„ ê³„íš

### Phase 3: Optimization (ì„ íƒì‚¬í•­)

1. â³ **ë©”ëª¨ë¦¬ ìµœì í™”**
   - Chunk ë‹¨ìœ„ ì²˜ë¦¬
   - ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì›

2. â³ **ë³‘ë ¬ ì²˜ë¦¬**
   - multiprocessing í™œìš©
   - ë‹¤ì¤‘ íŒŒì¼ ë™ì‹œ ì²˜ë¦¬

3. â³ **ê³ ê¸‰ ë³´ê°„ ê¸°ë²•**
   - Kalman filter
   - LSTM ê¸°ë°˜ ë³´ê°„

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- **ì‚¬ìš© ê°€ì´ë“œ**: [MASKED_PREPROCESSING_USAGE.md](MASKED_PREPROCESSING_USAGE.md)
- **í”„ë¡œì íŠ¸ ê°œì„ **: [IMPROVEMENTS.md](IMPROVEMENTS.md)
- **ë©”ì¸ README**: [README.md](README.md)

---

**Last Updated**: 2025-11-16
**Status**: âœ… All Critical Issues Resolved
**Version**: 2.0.0 (ë§ˆìŠ¤í‚¹ ì „ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ)

**ì£¼ìš” ê¸°ì—¬ì**: Claude Code
**ë¦¬ë·°ì–´**: -
**ìŠ¹ì¸**: -
