# ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œ ë¦¬ë·° ë° ê°œì„  ê³„íš

**ì‘ì„±ì¼**: 2025-11-16
**íŒŒì¼**: `src/preprocess.py`
**ìƒíƒœ**: ğŸ”´ Critical Issues Found

---

## ğŸ“Š í˜„ì¬ ì½”ë“œ ë¶„ì„

### ì „ì²˜ë¦¬ íë¦„

```
CSV íŒŒì¼ (loops*.csv, ~100ë§Œ í–‰)
    â†“
1. load_csv_data()              - DataFrame ë¡œë“œ
    â†“
2. create_node_mapping()        - ë…¸ë“œ ë§¤í•‘ ìƒì„± (raw_id or det_pos)
    â†“
3. create_time_index()          - ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
    â†“
4. convert_to_tensor()          - (T, N, F) í…ì„œ ë³€í™˜ âš ï¸ ë¬¸ì œ!
    â†“
5. interpolate_missing_speed()  - harmonicMeanSpeed ë³´ê°„ âš ï¸ ë¬¸ì œ!
    â†“
6. split_data()                 - Train/Val/Test ë¶„í•  (70/15/15)
    â†“
7. normalize_data()             - Z-score ì •ê·œí™” âš ï¸ ë¬¸ì œ!
    â†“
8. Save to .npz                 - ì €ì¥
```

---

## ğŸ”´ Critical Issues (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)

### Issue #1: det_pos ëª¨ë“œì—ì„œ ë°ì´í„° ë®ì–´ì“°ê¸°

**ìœ„ì¹˜**: `convert_to_tensor()`, Line 101-123

**ë¬¸ì œ**:
```python
for _, row in df.iterrows():
    if NODE_MODE == "det_pos":
        node_id = f"det_pos_{row['det_pos']}"
    # ...
    X[t_idx, n_idx, f_idx] = val  # âŒ ê°™ì€ ìœ„ì¹˜ ì—¬ëŸ¬ ì°¨ì„  â†’ ë§ˆì§€ë§‰ ê²ƒë§Œ ì €ì¥
```

**ì˜í–¥**:
- det_pos=0ì— 3ê°œ ì°¨ì„ (lane_idx=0,1,2)ì´ ìˆìœ¼ë©´ lane_idx=2ë§Œ ì €ì¥ë¨
- ì‹¤ì œë¡œëŠ” ì°¨ì„ ë³„ ë°ì´í„°ë¥¼ ì§‘ê³„í•´ì•¼ í•¨

**ì˜ˆì‹œ**:
```
det_pos=0, lane_idx=0: flow=10, occupancy=0.3
det_pos=0, lane_idx=1: flow=15, occupancy=0.4
det_pos=0, lane_idx=2: flow=12, occupancy=0.35

í˜„ì¬ ì½”ë“œ: flow=12, occupancy=0.35 (ë§ˆì§€ë§‰ ê²ƒë§Œ)
ì˜¬ë°”ë¥¸ ì²˜ë¦¬: flow=37 (í•©), occupancy=0.35 (í‰ê· )
```

**í•´ê²° ë°©ì•ˆ**:
```python
# Before: iterrowsë¡œ ë®ì–´ì“°ê¸°
for _, row in df.iterrows():
    X[t_idx, n_idx, f_idx] = val

# After: groupbyë¡œ ì§‘ê³„
if NODE_MODE == "det_pos":
    grouped = df.groupby(['begin', 'det_pos']).agg({
        'flow': 'sum',           # êµí†µëŸ‰ì€ í•©ì‚°
        'occupancy': 'mean',     # ì ìœ ìœ¨ì€ í‰ê· 
        'harmonicMeanSpeed': 'mean'  # ì†ë„ëŠ” í‰ê· 
    })
```

**ìš°ì„ ìˆœìœ„**: ğŸ”´ High (ë°ì´í„° ì •í™•ì„± ë¬¸ì œ)

---

### Issue #2: iterrows() ì„±ëŠ¥ ë¬¸ì œ

**ìœ„ì¹˜**: `convert_to_tensor()`, Line 101

**ë¬¸ì œ**:
- iterrows()ëŠ” row-by-row iterationìœ¼ë¡œ ë§¤ìš° ëŠë¦¼
- 100ë§Œ í–‰ ì²˜ë¦¬ ì‹œ **10-30ë¶„** ì†Œìš” ì˜ˆìƒ

**ë²¤ì¹˜ë§ˆí¬**:
```python
# iterrows() ë°©ì‹
for _, row in df.iterrows():  # ~30ë¶„
    X[t_idx, n_idx, f_idx] = row[feature]

# vectorized ë°©ì‹
pivot = df.pivot_table(...)   # ~1-5ì´ˆ (600ë°° ë¹ ë¦„!)
X[:, :, f_idx] = pivot.values
```

**í•´ê²° ë°©ì•ˆ**:
```python
def convert_to_tensor_fast(df, node_to_idx, features):
    """
    Pivot tableì„ ì‚¬ìš©í•œ ë¹ ë¥¸ ë³€í™˜
    """
    # ë…¸ë“œ ì»¬ëŸ¼ ì„¤ì •
    node_col = 'raw_id' if NODE_MODE == 'raw_id' else 'det_pos'

    # ê° featureë³„ë¡œ pivot
    tensor_list = []
    for feature in features:
        pivot = df.pivot_table(
            values=feature,
            index='begin',
            columns=node_col,
            aggfunc='mean' if NODE_MODE == 'raw_id' else
                   ('sum' if feature == 'flow' else 'mean')
        )
        tensor_list.append(pivot.values)

    # (T, N, F) í˜•íƒœë¡œ stack
    X = np.stack(tensor_list, axis=2)
    return X
```

**ìš°ì„ ìˆœìœ„**: ğŸ”´ High (ì‚¬ìš©ì ê²½í—˜ ì‹¬ê° ì €í•˜)

---

### Issue #3: flow/occupancy ê²°ì¸¡ê°’ ë¯¸ì²˜ë¦¬

**ìœ„ì¹˜**: `interpolate_missing_speed()`, Line 129-182

**ë¬¸ì œ**:
- harmonicMeanSpeedë§Œ ë³´ê°„
- flow, occupancyëŠ” NaN ê·¸ëŒ€ë¡œ ë°©ì¹˜
- ëª¨ë¸ í•™ìŠµ ì‹œ NaNìœ¼ë¡œ ì¸í•œ ì—ëŸ¬ ë˜ëŠ” ì„±ëŠ¥ ì €í•˜

**ë°ì´í„° í™•ì¸ í•„ìš”**:
```python
# CSVì—ì„œ ë¹ˆ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
df['flow'].isna().sum()        # ?
df['occupancy'].isna().sum()   # ?
```

**í•´ê²° ë°©ì•ˆ**:
```python
def interpolate_all_features(X: np.ndarray) -> np.ndarray:
    """
    ëª¨ë“  íŠ¹ì„±ì— ëŒ€í•´ ê²°ì¸¡ê°’ ë³´ê°„
    """
    X_interp = X.copy()

    for f_idx in range(X.shape[2]):  # ê° íŠ¹ì„±
        for n in range(X.shape[1]):  # ê° ë…¸ë“œ
            series = X[:, n, f_idx]

            if np.all(np.isnan(series)):
                # ëª¨ë“  ê°’ì´ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ
                X_interp[:, n, f_idx] = 0
                continue

            # ì‹œê³„ì—´ ì„ í˜• ë³´ê°„
            interpolated = pd.Series(series).interpolate(
                method='linear',
                limit_direction='both',
                fill_value=0
            )
            X_interp[:, n, f_idx] = interpolated.values

    return X_interp
```

**ìš°ì„ ìˆœìœ„**: ğŸ”´ High (ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ ê°€ëŠ¥)

---

## ğŸŸ¡ Important Issues (ë‹¨ê¸° ê°œì„ )

### Issue #4: ê²°ì¸¡ê°’ ì¶”ë¡  ë¡œì§ ë²„ê·¸

**ìœ„ì¹˜**: `interpolate_missing_speed()`, Line 162-163

**ë¬¸ì œ**:
```python
# flow/occupancyê°€ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ì„¤ì •
flow_val = X[t, n, flow_idx] if not np.isnan(X[t, n, flow_idx]) else 0
occ_val = X[t, n, occ_idx] if not np.isnan(X[t, n, occ_idx]) else 0

# ë¬¸ì œ: NaN â†’ 0 â†’ "no vehicles" ì¡°ê±´ ì¶©ì¡± â†’ ë¶€ì ì ˆí•œ ì¶”ë¡ 
if flow_val < 0.1 and occ_val < 0.1:
    X_interp[t, n, speed_idx] = FREE_FLOW_SPEED
```

**í•´ê²° ë°©ì•ˆ**:
```python
# NaNì´ë©´ ê±´ë„ˆë›°ê¸°
if np.isnan(X[t, n, flow_idx]) or np.isnan(X[t, n, occ_idx]):
    # í•´ë‹¹ ë…¸ë“œì˜ í‰ê·  ì‚¬ìš©
    valid_speeds = speed_series[~np.isnan(speed_series)]
    X_interp[t, n, speed_idx] = np.mean(valid_speeds) if len(valid_speeds) > 0 else FREE_FLOW_SPEED
    continue

flow_val = X[t, n, flow_idx]
occ_val = X[t, n, occ_idx]
# ... ê¸°ì¡´ ë¡œì§
```

**ìš°ì„ ìˆœìœ„**: ğŸŸ¡ Medium

---

### Issue #5: ë°ì´í„° ê²€ì¦ ë¶€ì¡±

**ìœ„ì¹˜**: ì „ì²´

**ë¬¸ì œ**:
- ì…ë ¥ ë°ì´í„° ê²€ì¦ ì—†ìŒ
- ë³€í™˜ í›„ ë°ì´í„° ê²€ì¦ ì—†ìŒ
- ì—ëŸ¬ ë°œìƒ ì‹œ ì›ì¸ íŒŒì•… ì–´ë ¤ì›€

**ì¶”ê°€ í•„ìš”**:
```python
def validate_input_data(df: pd.DataFrame) -> None:
    """ì…ë ¥ CSV ë°ì´í„° ê²€ì¦"""
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['begin', 'end', 'raw_id', 'det_pos', 'flow',
                     'occupancy', 'harmonicMeanSpeed']
    missing = set(required_cols) - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns: {missing}")

    # 2. ì‹œê°„ ì—°ì†ì„± í™•ì¸
    time_steps = sorted(df['begin'].unique())
    time_diff = np.diff(time_steps)
    if not np.allclose(time_diff, TIME_STEP_SIZE):
        warnings.warn("Non-uniform time steps detected")

    # 3. ê°’ ë²”ìœ„ í™•ì¸
    if df['flow'].min() < 0:
        raise ValueError("flow cannot be negative")

    if not df['occupancy'].between(0, 1, inclusive='both').all():
        warnings.warn("occupancy values outside [0, 1] range")

def validate_tensor(X: np.ndarray, name: str) -> None:
    """í…ì„œ ê²€ì¦"""
    # 1. Shape í™•ì¸
    assert X.ndim == 3, f"{name} must be 3D (T, N, F)"

    # 2. NaN í™•ì¸
    nan_count = np.isnan(X).sum()
    if nan_count > 0:
        raise ValueError(f"{name} contains {nan_count} NaN values")

    # 3. Inf í™•ì¸
    if np.any(np.isinf(X)):
        raise ValueError(f"{name} contains inf values")

    print(f"âœ“ {name} validation passed: shape={X.shape}")
```

**ìš°ì„ ìˆœìœ„**: ğŸŸ¡ Medium

---

### Issue #6: ì •ê·œí™” ì „ NaN ê²€ì¦ ë¶€ì¡±

**ìœ„ì¹˜**: `normalize_data()`, Line 185-217

**ë¬¸ì œ**:
```python
# np.nanmean ì‚¬ìš© â†’ NaNì´ ìˆì–´ë„ ì—ëŸ¬ ì—†ì´ ì§„í–‰
mean = np.nanmean(train_feat)
std = np.nanstd(train_feat)

# ë¬¸ì œ: NaNì´ ë§ìœ¼ë©´ í†µê³„ê°€ ë¶€ì •í™•
```

**í•´ê²° ë°©ì•ˆ**:
```python
def normalize_data(X_train, X_val, X_test):
    """
    Z-score normalization with strict validation
    """
    # NaN ê²€ì¦ (ì •ê·œí™” ì „ì— ëª¨ë“  NaN ì œê±°ë˜ì–´ì•¼ í•¨)
    for split_name, split_data in [('train', X_train), ('val', X_val), ('test', X_test)]:
        nan_count = np.isnan(split_data).sum()
        if nan_count > 0:
            raise ValueError(f"{split_name} contains {nan_count} NaN values before normalization")

    stats = {}
    # ... ê¸°ì¡´ ë¡œì§ (np.mean ì‚¬ìš©, np.nanmean ì•„ë‹˜)
```

**ìš°ì„ ìˆœìœ„**: ğŸŸ¡ Medium

---

## ğŸŸ¢ Nice to Have (ì¥ê¸° ê°œì„ )

### Issue #7: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

**í˜„ì¬**: ì „ì²´ DataFrameì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ (~100MB+)

**ê°œì„  ë°©ì•ˆ**:
```python
# Chunk ë‹¨ìœ„ ì²˜ë¦¬
chunks = []
for chunk in pd.read_csv(csv_path, chunksize=50000):
    processed = process_chunk(chunk)
    chunks.append(processed)
X = np.concatenate(chunks, axis=0)
```

**ìš°ì„ ìˆœìœ„**: ğŸŸ¢ Low

---

### Issue #8: ë³‘ë ¬ ì²˜ë¦¬ ë¶€ì¬

**ê°œì„  ë°©ì•ˆ**:
```python
from multiprocessing import Pool

def process_node(n):
    """ë‹¨ì¼ ë…¸ë“œ ë³´ê°„"""
    # ...

# ë³‘ë ¬ ì²˜ë¦¬
with Pool(processes=4) as pool:
    results = pool.map(process_node, range(num_nodes))
```

**ìš°ì„ ìˆœìœ„**: ğŸŸ¢ Low

---

## ğŸ“‹ ê°œì„  ê³„íš

### Phase 1: Critical Fixes (ì¦‰ì‹œ)

**ëª©í‘œ**: ê¸°ëŠ¥ì  ë¬¸ì œ í•´ê²°

1. âœ… **vectorized convert_to_tensor êµ¬í˜„**
   - pivot_table ì‚¬ìš©
   - 600ë°° ì„±ëŠ¥ í–¥ìƒ
   - ì˜ˆìƒ ì†Œìš”: 1-2ì‹œê°„

2. âœ… **det_pos ëª¨ë“œ ì§‘ê³„ ìˆ˜ì •**
   - groupby + agg ì‚¬ìš©
   - flow: sum, occupancy/speed: mean
   - ì˜ˆìƒ ì†Œìš”: 30ë¶„

3. âœ… **ëª¨ë“  íŠ¹ì„± ê²°ì¸¡ê°’ ì²˜ë¦¬**
   - interpolate_all_features() êµ¬í˜„
   - 3ê°œ íŠ¹ì„± ëª¨ë‘ ë³´ê°„
   - ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„

4. âœ… **ë°ì´í„° ê²€ì¦ ì¶”ê°€**
   - validate_input_data()
   - validate_tensor()
   - ì˜ˆìƒ ì†Œìš”: 1ì‹œê°„

### Phase 2: Quality Improvements (ë‹¨ê¸°)

**ëª©í‘œ**: ì•ˆì •ì„± ë° ì‹ ë¢°ì„± í–¥ìƒ

5. âœ… **ê²°ì¸¡ê°’ ì¶”ë¡  ë¡œì§ ìˆ˜ì •**
   - NaN ì²˜ë¦¬ ë²„ê·¸ ìˆ˜ì •
   - ì˜ˆìƒ ì†Œìš”: 30ë¶„

6. âœ… **ì •ê·œí™” ê²€ì¦ ê°•í™”**
   - NaN ì²´í¬ ì¶”ê°€
   - ì˜ˆìƒ ì†Œìš”: 30ë¶„

7. âœ… **ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¶”ê°€**
   - test_preprocess.py
   - ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‘ì„±
   - ì˜ˆìƒ ì†Œìš”: 2ì‹œê°„

### Phase 3: Optimization (ì¥ê¸°)

**ëª©í‘œ**: ì„±ëŠ¥ ë° í™•ì¥ì„±

8. â³ **ë©”ëª¨ë¦¬ ìµœì í™”**
   - Chunk ì²˜ë¦¬
   - ì˜ˆìƒ ì†Œìš”: 2ì‹œê°„

9. â³ **ë³‘ë ¬ ì²˜ë¦¬ ì¶”ê°€**
   - multiprocessing
   - ì˜ˆìƒ ì†Œìš”: 2-3ì‹œê°„

---

## ğŸ¯ ìˆ˜ì • í›„ ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | Before | After | ê°œì„  |
|------|--------|-------|------|
| ì²˜ë¦¬ ì‹œê°„ (100ë§Œ í–‰) | ~30ë¶„ | ~5ì´ˆ | **360ë°°** â†‘ |
| det_pos ì •í™•ë„ | ë°ì´í„° ì†ì‹¤ | ì˜¬ë°”ë¥¸ ì§‘ê³„ | âœ… |
| ê²°ì¸¡ê°’ ì²˜ë¦¬ | speedë§Œ | ëª¨ë“  íŠ¹ì„± | âœ… |
| ë°ì´í„° ê²€ì¦ | ì—†ìŒ | ì™„ì „ ê²€ì¦ | âœ… |
| í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ | 0% | ~80% | âœ… |
| ì—ëŸ¬ ë””ë²„ê¹… | ì–´ë ¤ì›€ | ëª…í™•í•œ ë©”ì‹œì§€ | âœ… |

---

## ğŸ“ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 êµ¬í˜„ í•­ëª©

- [ ] `convert_to_tensor_vectorized()` í•¨ìˆ˜ ì‘ì„±
- [ ] det_pos ëª¨ë“œ ì§‘ê³„ ë¡œì§ ì¶”ê°€
- [ ] `interpolate_all_features()` í•¨ìˆ˜ ì‘ì„±
- [ ] `validate_input_data()` í•¨ìˆ˜ ì¶”ê°€
- [ ] `validate_tensor()` í•¨ìˆ˜ ì¶”ê°€
- [ ] ê¸°ì¡´ `process_single_file()` í•¨ìˆ˜ ì—…ë°ì´íŠ¸
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ ê°œì„ 
- [ ] ì§„í–‰ ìƒí™© ë¡œê¹… ì¶”ê°€

### í…ŒìŠ¤íŠ¸ í•­ëª©

- [ ] test_convert_to_tensor_raw_id()
- [ ] test_convert_to_tensor_det_pos()
- [ ] test_interpolate_all_features()
- [ ] test_validate_input_data()
- [ ] test_normalize_data()
- [ ] test_split_data()
- [ ] test_full_pipeline()

### ë¬¸ì„œí™”

- [ ] docstring ì¶”ê°€/ê°œì„ 
- [ ] READMEì— ì „ì²˜ë¦¬ ê°€ì´ë“œ ì¶”ê°€
- [ ] ì˜ˆì œ ë…¸íŠ¸ë¶ ì‘ì„±

---

## ğŸ” ì°¸ê³  ì‚¬í•­

### ë°ì´í„° íŠ¹ì„±

- **ì‹œê°„ ê°„ê²©**: 5ì´ˆ
- **ë…¸ë“œ ìˆ˜**: 480 (raw_id) ë˜ëŠ” 160 (det_pos)
- **íŠ¹ì„±**: flow, occupancy, harmonicMeanSpeed
- **CSV í¬ê¸°**: ~75MB per file
- **í–‰ ìˆ˜**: ~100ë§Œ í–‰

### ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„°

```python
# config.py
NODE_MODE = "raw_id"              # or "det_pos"
FEATURES = ["flow", "occupancy", "harmonicMeanSpeed"]
TIME_STEP_SIZE = 5.0              # seconds
MISSING_SPEED_VALUE = -1.0
FREE_FLOW_SPEED = 15.0            # m/s
CONGESTED_SPEED = 2.5             # m/s
TRAIN_RATIO = 0.7
VAL_RATIO = 0.15
TEST_RATIO = 0.15
```

---

**Last Updated**: 2025-11-16
**Status**: Ready for Implementation
**Estimated Total Time**: Phase 1 = 4-5 hours
